# Sets the base image for subsequent instructions
FROM nvcr.io/nvidia/pytorch:24.09-py3 AS base

# Sets labels for the image
LABEL org.opencontainers.image.source="https://github.com/entelecheia/TensorRT-LLM-container"
LABEL org.opencontainers.image.description="Containerized solution for optimized LLM inference using TensorRT-LLM on NVIDIA GPUs."
LABEL org.opencontainers.image.licenses="MIT"

# https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html
# The default values come from `nvcr.io/nvidia/pytorch`
ENV BASH_ENV=${BASH_ENV:-/etc/bash.bashrc}
ENV ENV=${ENV:-/etc/shinit_v2}
SHELL ["/bin/bash", "-c"]

FROM base as devel

# Setting this argument prevents interactive prompts during the build process
ARG DEBIAN_FRONTEND=noninteractive
# Updates the image and installs necessary packages
RUN apt-get update --fix-missing \
    && apt-get install -y curl wget jq sudo gosu build-essential software-properties-common \
    locales locales-all fontconfig fonts-nanum \
    tzdata openssh-server

COPY .docker/common/install_base.sh install_base.sh
RUN bash ./install_base.sh && rm install_base.sh

COPY .docker/common/install_cmake.sh install_cmake.sh
RUN bash ./install_cmake.sh && rm install_cmake.sh

COPY .docker/common/install_ccache.sh install_ccache.sh
RUN bash ./install_ccache.sh && rm install_ccache.sh

# Download & install internal TRT release
ARG TRT_VER
ARG CUDA_VER
ARG CUDNN_VER
ARG NCCL_VER
ARG CUBLAS_VER
COPY .docker/common/install_tensorrt.sh install_tensorrt.sh
RUN bash ./install_tensorrt.sh \
    --TRT_VER=${TRT_VER} \
    --CUDA_VER=${CUDA_VER} \
    --CUDNN_VER=${CUDNN_VER} \
    --NCCL_VER=${NCCL_VER} \
    --CUBLAS_VER=${CUBLAS_VER} && \
    rm install_tensorrt.sh

# Install latest Polygraphy
COPY .docker/common/install_polygraphy.sh install_polygraphy.sh
RUN bash ./install_polygraphy.sh && rm install_polygraphy.sh

# Install mpi4py
COPY .docker/common/install_mpi4py.sh install_mpi4py.sh
RUN bash ./install_mpi4py.sh && rm install_mpi4py.sh

# Install PyTorch
ARG TORCH_INSTALL_TYPE="skip"
COPY .docker/common/install_pytorch.sh install_pytorch.sh
RUN bash ./install_pytorch.sh $TORCH_INSTALL_TYPE && rm install_pytorch.sh

FROM devel as wheel
WORKDIR /src/tensorrt_llm
COPY src/TensorRT-LLM/benchmarks benchmarks
COPY src/TensorRT-LLM/cpp cpp
COPY src/TensorRT-LLM/benchmarks benchmarks
COPY src/TensorRT-LLM/scripts scripts
COPY src/TensorRT-LLM/tensorrt_llm tensorrt_llm
COPY src/TensorRT-LLM/3rdparty 3rdparty
COPY src/TensorRT-LLM/setup.py src/TensorRT-LLM/requirements.txt src/TensorRT-LLM/requirements-dev.txt ./

# Create cache directories for pip and ccache
RUN mkdir -p /root/.cache/pip /root/.cache/ccache
ENV CCACHE_DIR=/root/.cache/ccache
# Build the TRT-LLM wheel
ARG BUILD_WHEEL_ARGS="--clean --trt_root /usr/local/tensorrt --python_bindings --benchmarks"
RUN --mount=type=cache,target=/root/.cache/pip --mount=type=cache,target=/root/.cache/ccache \
    python3 scripts/build_wheel.py ${BUILD_WHEEL_ARGS}

FROM devel as release    

# Create a cache directory for pip
RUN mkdir -p /root/.cache/pip

WORKDIR /app/tensorrt_llm
COPY --from=wheel /src/tensorrt_llm/build/tensorrt_llm*.whl .
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install tensorrt_llm*.whl --extra-index-url https://pypi.nvidia.com && \
    rm tensorrt_llm*.whl
COPY src/TensorRT-LLM/README.md ./
COPY src/TensorRT-LLM/docs docs
COPY src/TensorRT-LLM/cpp/include include
RUN ln -sv $(python3 -c 'import site; print(f"{site.getsitepackages()[0]}/tensorrt_llm/bin")') bin && \
    test -f bin/executorWorker && \
    ln -sv $(python3 -c 'import site; print(f"{site.getsitepackages()[0]}/tensorrt_llm/libs")') lib && \
    test -f lib/libnvinfer_plugin_tensorrt_llm.so && \
    ln -sv lib/libnvinfer_plugin_tensorrt_llm.so lib/libnvinfer_plugin_tensorrt_llm.so.9 && \
    echo "/app/tensorrt_llm/lib" > /etc/ld.so.conf.d/tensorrt_llm.conf && \
    ldconfig
ARG SRC_DIR=/src/tensorrt_llm
COPY --from=wheel ${SRC_DIR}/benchmarks benchmarks
ARG CPP_BUILD_DIR=${SRC_DIR}/cpp/build
COPY --from=wheel \
     ${CPP_BUILD_DIR}/benchmarks/bertBenchmark \
     ${CPP_BUILD_DIR}/benchmarks/gptManagerBenchmark \
     ${CPP_BUILD_DIR}/benchmarks/gptSessionBenchmark \
     benchmarks/cpp/
COPY src/TensorRT-LLM/examples examples
RUN chmod -R a+w examples && \
    rm -v \
      benchmarks/cpp/bertBenchmark.cpp \
      benchmarks/cpp/gptManagerBenchmark.cpp \
      benchmarks/cpp/gptSessionBenchmark.cpp \
      benchmarks/cpp/CMakeLists.txt
ARG TRT_GIT_COMMIT
ARG TRT_LLM_VER
ENV TRT_LLM_GIT_COMMIT=${TRT_GIT_COMMIT} \
    TRT_LLM_VERSION=${TRT_LLM_VER}

# Sets Python environment variables
ENV PIP_DEFAULT_TIMEOUT 100
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Setting ARGs and ENVs for the app
ARG ARG_APP_INSTALL_ROOT="/opt"
ARG ARG_APP_DIRNAME="entelecheia"
ENV APP_INSTALL_ROOT $ARG_APP_INSTALL_ROOT
ENV APP_DIRNAME $ARG_APP_DIRNAME
ENV APP_SRC_DIR=${APP_INSTALL_ROOT}/${APP_DIRNAME}
ENV APP_VIRTUAL_ENV=${APP_INSTALL_ROOT}/.venvs/${APP_DIRNAME}
ENV PATH="$APP_VIRTUAL_ENV/bin:$PATH"
ENV APP_WORKSPACE_ROOT=${APP_INSTALL_ROOT}/workspace
ARG ARG_WORKSPACE_ROOT="/workspace"
ENV WORKSPACE_ROOT $ARG_WORKSPACE_ROOT
# Sets up the workspace for the user
RUN mkdir -p $WORKSPACE_ROOT/projects

# Sets the working directory to workspace root
WORKDIR $WORKSPACE_ROOT
# Copies scripts from host into the image
COPY ./.docker/scripts/ ./scripts/
# RUN pip install -r ./scripts/requirements-dev.txt

# Sets the time zone within the container
ENV TZ="Asia/Seoul"
# Sets up the locale to en_US.UTF-8
RUN localedef -v -c -i en_US -f UTF-8 en_US.UTF-8 || true

# Setting ARGs and ENVs for user creation and workspace setup
ARG ARG_USERNAME="dev"
ARG ARG_USER_UID=9001
ARG ARG_USER_GID=$ARG_USER_UID
ENV USERNAME $ARG_USERNAME
ENV USER_UID $ARG_USER_UID
ENV USER_GID $ARG_USER_GID

# Creates a non-root user with sudo privileges
# check if user exists and if not, create user
RUN if id -u $USERNAME >/dev/null 2>&1; then \
        # if the current user's user id is different from the specified user id, change the user id of the current user to the specified user id
        if [ "$USER_UID" -ne "$(id -u $USERNAME)" ]; then \
            usermod --uid $USER_UID $USERNAME; \
            chown --recursive $USER_UID:$USER_UID $WORKSPACE_ROOT; \
            chown --recursive $USER_UID:$USER_UID $APP_INSTALL_ROOT; \
        fi; \
    else \
        groupadd --gid $USER_GID $USERNAME && \
        adduser --uid $USER_UID --gid $USER_GID --force-badname --disabled-password --gecos "" $USERNAME && \
        echo "$USERNAME:$USERNAME" | chpasswd && \
        adduser $USERNAME sudo && \
        echo "$USERNAME ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers && \
        echo "$USERNAME ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/$USERNAME && \
        chmod 0440 /etc/sudoers.d/$USERNAME; \
    fi

USER root

# Copies entrypoint script from host into the image
COPY ./.docker/scripts/entrypoint.sh /usr/local/bin/entrypoint.sh
# Changes the entrypoint script permissions to make it executable
RUN chmod +x /usr/local/bin/entrypoint.sh
# Sets the entrypoint script as the default command that will be executed when the container is run
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]
    
# Specifies the command that will be executed when the container is run
CMD ["bash"]
