# Changes here will be overwritten by Copier; do NOT edit manually
_commit: v0.30.2
_src_path: gh:entelecheia/hyperfast-docker-template
app_dirname: entelecheia
app_install_root: /opt
author: Young Joon Lee
build_images_from_dockerfile: true
container_service_name: app
container_workspace_root: /workspace
copy_scripts_dir: true
cuda_device_id: all
docker_apt_packages: fontconfig fonts-nanum
docker_build_from: nvidia/cuda:12.5.1-devel-ubuntu22.04
docker_container_uid: 9001
docker_container_username: dev
docker_image_variant_name: dev
docker_image_version_variable_name: IMAGE_VERSION
docker_name_prefix: TRT_LLM
docker_project_name: tensorrt-llm
docker_registry: ghcr.io
docker_run_command: zsh
docker_service_name: workspace
docker_timezone: Asia/Seoul
docker_username: entelecheia
email: entelecheia@hotmail.com
enable_nvidia_gpu: true
friendly_name: "Container for TennsorRT-LLM"
github_repo_name: TensorRT-LLM-container
github_username: entelecheia
install_dotfiles: false
jupyter_host_port: 18598
jupyter_port: 8585
jupyter_token: __juypter_token_(change_me)__
launch_scripts: launch.sh
main_branch: main
project_description: TensorRT-LLM-container provides a containerized environment for TensorRT-LLM, enabling efficient development and deployment of Large Language Models (LLMs) on NVIDIA GPUs. It encapsulates TensorRT-LLM's Python API for model definition, TensorRT engine building with advanced optimizations, and runtime components for both Python and C++.
project_license: MIT
project_short_description: Containerized solution for optimized LLM inference using TensorRT-LLM on NVIDIA GPUs.
ssh_host_port: 2253
ssh_port: 22
use_deploy_workflows: true
use_jupyter: false
use_semantic_versioning_for_image: true
use_ssh_service: true
use_web_service: true
web_service_host_port: 19590
web_service_port: 8080
